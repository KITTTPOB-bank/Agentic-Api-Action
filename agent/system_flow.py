 
from langgraph.graph import StateGraph , START, END
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, SystemMessage ,AIMessage, ToolMessage
import os
from .structured import MessageStatus, MessagesFlow
from .prompt import PROMPT_ANALYZE , PROMPT_TOOL_AGENT
from langchain_google_genai import ChatGoogleGenerativeAI
from dotenv import load_dotenv
from langchain_core.prompts import PromptTemplate
import datetime
import base64
load_dotenv()

def call_process(tools : list , Meme: bool):
    GPT_KEY = os.getenv("GPT_KEY")
 
    GEMINI_KEY = os.getenv("GEMINI_KEY")
    tools_by_name = {tool.name: tool for tool in tools}
    llm = ChatOpenAI(model="gpt-4o-mini",  temperature=0.3 , api_key=GPT_KEY)

    def call_generate(state: MessagesFlow):
        messages = state["messages"]
 
        meme_msg = llm.invoke(f"""
‡∏†‡∏≤‡∏£‡∏Å‡∏¥‡∏à‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠:  **‡∏Ñ‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡πÅ‡∏°‡∏ß‡∏ó‡∏µ‡πà‡πÅ‡∏õ‡∏•‡∏Å ‡∏Æ‡∏≤ ‡∏´‡∏•‡∏∏‡∏î‡πÇ‡∏•‡∏Å ‡πÅ‡∏•‡∏∞‡∏Ç‡∏≠‡∏†‡∏≤‡∏û‡∏ô‡∏±‡πâ‡∏ô** ‡πÇ‡∏î‡∏¢‡∏≠‡∏¥‡∏á‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á ‡∏™‡πà‡∏á‡∏Ñ‡∏∑‡∏ô‡πÅ‡∏Ñ‡πà 1 ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°
üìÑ ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°:
{messages[0].content}

üéØ ‡πÅ‡∏ô‡∏ß‡∏ó‡∏≤‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏°‡∏µ‡∏°:
- ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡πÅ‡∏°‡∏ß‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏†‡∏≤‡∏û‡πÄ‡∏™‡∏°‡∏≠
- ‡∏ñ‡πâ‡∏≤‡∏û‡∏π‡∏î‡∏ñ‡∏∂‡∏á‡∏ó‡∏≠‡∏á ‚Üí ‡πÉ‡∏´‡πâ‡πÅ‡∏°‡∏ß‡∏ñ‡∏∑‡∏≠‡∏ó‡∏≠‡∏á / ‡∏≠‡∏≤‡∏ö‡∏ó‡∏≠‡∏á / ‡∏Å‡∏•‡∏¥‡πâ‡∏á‡∏ö‡∏ô‡∏ó‡∏≠‡∏á
- ‡∏ñ‡πâ‡∏≤‡∏û‡∏π‡∏î‡∏ñ‡∏∂‡∏á‡∏´‡∏∏‡πâ‡∏ô‡πÅ‡∏≠‡∏õ‡πÄ‡∏õ‡∏¥‡πâ‡∏• ‚Üí ‡πÉ‡∏´‡πâ‡πÅ‡∏°‡∏ß‡∏Å‡∏±‡∏î‡πÅ‡∏≠‡∏õ‡πÄ‡∏õ‡∏¥‡πâ‡∏• ‡∏´‡∏£‡∏∑‡∏≠‡∏õ‡∏µ‡∏ô‡∏ï‡πâ‡∏ô‡πÅ‡∏≠‡∏õ‡πÄ‡∏õ‡∏¥‡πâ‡∏•
- ‡∏ñ‡πâ‡∏≤‡πÄ‡∏®‡∏£‡∏©‡∏ê‡∏Å‡∏¥‡∏à‡πÅ‡∏¢‡πà ‚Üí ‡πÉ‡∏´‡πâ‡πÅ‡∏°‡∏ß‡πÉ‡∏™‡πà‡∏™‡∏π‡∏ó‡∏î‡∏π‡πÄ‡∏®‡∏£‡πâ‡∏≤ ‡∏´‡∏£‡∏∑‡∏≠‡∏ñ‡∏∑‡∏≠‡∏Å‡∏£‡∏≤‡∏ü‡∏ï‡∏Å
                                             
‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå ‡∏ó‡∏µ‡πà‡∏Ñ‡∏≤‡∏î‡∏´‡∏ß‡∏±‡∏á‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡πâ‡∏ô‡πÜ‡πÄ‡∏ä‡πà‡∏ô `‡∏Ç‡∏≠‡∏†‡∏≤‡∏û‡πÅ‡∏°‡∏ß‡πÉ‡∏™‡πà‡∏™‡∏£‡πâ‡∏≠‡∏á‡∏ó‡∏≠‡∏á‡∏¢‡∏¥‡πà‡∏°` , `‡∏Ç‡∏≠‡∏†‡∏≤‡∏û‡πÅ‡∏°‡∏ß‡πÄ‡∏•‡∏µ‡∏¢‡πÅ‡∏≠‡∏õ‡πÄ‡∏õ‡∏¥‡πâ‡∏•` ‡πÅ‡∏Ñ‡πà 1 ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô""")
                
        message = {
    "role": "user",
    "content":  meme_msg.content
    }

        llm_gemini = ChatGoogleGenerativeAI(
            model="models/gemini-2.0-flash-exp-image-generation",
            temperature=0.7,
            max_tokens=None,
            api_key=GEMINI_KEY
        )

        try:
            response = llm_gemini.invoke(
                [message],
                generation_config=dict(response_modalities=["TEXT", "IMAGE"]),
            )
    
            image_base64 = response.content[0].get("image_url").get("url").split(",")[-1]

            image_data = base64.b64decode(image_base64)

            folder_path = "generated_images"
            os.makedirs(folder_path, exist_ok=True)

            filename = f"meme_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.png"
            file_path = os.path.join(folder_path, filename)
            with open(file_path, "wb") as f:
                    f.write(image_data)
        except:
            print("‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡πÑ‡∏î‡πâ‡∏≠‡∏≤‡∏à‡πÄ‡∏Å‡∏¥‡∏î‡∏à‡∏≤‡∏Å Key ‡∏°‡∏µ‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á")
            return None

        return None
    
    def call_analyze(state: MessagesFlow):
        messages = state["messages"]

        prompt_template = PromptTemplate.from_template(PROMPT_ANALYZE)
        prompt = prompt_template.format(date_now=datetime.datetime.now().strftime("%Y-%m-%d"))
     
        llm_with_ouput = llm.with_structured_output(MessageStatus)
        message = llm_with_ouput.invoke([SystemMessage(content=prompt)] + messages)
        return {"messages": AIMessage(content=message.message), "status": message.status}
    

    def should_end(state: MessagesFlow):
        status = state["status"]
        if status == "end":
            return END
        return "call_agent"
    
    def should_end_meme(state: MessagesFlow):
        status = state["status"]
        if status == "end":
            return "generate"
        return "call_agent"

    def should_continue(state: MessagesFlow):
        messages = state["messages"]
        last_message = messages[-1]
        if last_message.tool_calls:
            return "tools"

        return "call_analyze"

    def tool_node(state: dict):
        result = []
        for tool_call in state["messages"][-1].tool_calls:
            tool = tools_by_name[tool_call["name"]]
            observation = tool.invoke(tool_call["args"])
            result.append(ToolMessage(content=observation, tool_call_id=tool_call["id"], tools_by_name=tool_call["name"]))
        return {"messages": result}


    def call_model(state: MessagesFlow):
        messages = state["messages"]
        prompt_template = PromptTemplate.from_template(PROMPT_TOOL_AGENT)
        prompt = prompt_template.format(date_now=datetime.datetime.now().strftime("%Y-%m-%d"))

        model_with_tools = llm.bind_tools(tools)
        response = model_with_tools.invoke([SystemMessage(content=prompt)] + messages)

        return {"messages": [response]}


    workflow = StateGraph(MessagesFlow)

    workflow.add_node("call_agent", call_model)
    workflow.add_node("tools", tool_node)
    workflow.add_node("call_analyze", call_analyze)

    if Meme:
        workflow.add_node("generate", call_generate)

    workflow.add_edge(START, "call_analyze")
    if Meme:
        workflow.add_conditional_edges("call_analyze", should_end_meme, ["call_agent", "generate" ])
        workflow.add_conditional_edges("call_agent", should_continue, ["tools", "call_analyze"])
        workflow.add_edge("tools", "call_agent")
        workflow.add_edge("generate", END)
    else:
        workflow.add_conditional_edges("call_analyze", should_end, ["call_agent", END ])
        workflow.add_conditional_edges("call_agent", should_continue, ["tools", "call_analyze"])
        workflow.add_edge("tools", "call_agent")

    return workflow.compile()